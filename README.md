# DA6401
DL Assignments
DA6401 Assignment 1: Neural Network Implementation for Fashion-MNIST Classification
Project Overview
This repository contains the implementation of a flexible feedforward neural network trained and evaluated on the Fashion-MNIST dataset. Various optimization algorithms, hyperparameter tuning strategies, and visualizations using WandB are implemented.

Repository Structure
DLass1Q1.ipynb: Dataset visualization and initial analysis.
DLass1Q2.ipynb: Flexible feedforward neural network architecture implementation.
DLass1Q3.ipynb: Backpropagation implementation with optimizers (SGD, Momentum, NAG, RMSprop, Adam, Nadam).
DLass1Q4to6.ipynb: Hyperparameter tuning and WandB sweeps.
DLass1Q7.ipynb: Evaluation using a confusion matrix visualization.
DLass1Q8.ipynb: Comparison between cross-entropy and squared error losses.
DLass1Q10.ipynb: Applying insights from Fashion-MNIST to the MNIST digit recognition task.
Activations.ipynb: Exploration of various activation functions.
Loss.ipynb: Detailed experiments and comparisons for different loss functions.
Optimizers.ipynb: In-depth exploration of optimization algorithms.
README.md: Project description, setup, and execution instructions.


- [**`DLass1Q1.ipynb`**](https://github.com/SubikkshaS/da6401_assignment1/blob/main/DLass1Q1.ipynb): Dataset visualization and initial analysis.
- [**`DLass1Q2.ipynb`**](https://github.com/SubikkshaS/da6401_assignment1/blob/main/DLass1Q2.ipynb): Flexible feedforward neural network architecture implementation.
- [**`DLass1Q3.ipynb`**](https://github.com/SubikkshaS/da6401_assignment1/blob/main/DLass1Q3.ipynb): Backpropagation implementation with optimizers (SGD, Momentum, NAG, RMSprop, Adam, Nadam).
- [**`DLass1Q4to6.ipynb`**](https://github.com/SubikkshaS/da6401_assignment1/blob/main/DLass1Q4to6.ipynb): Hyperparameter tuning and WandB sweeps.
- [**`DLass1Q7.ipynb`**](https://github.com/SubikkshaS/da6401_assignment1/blob/main/DLass1Q7.ipynb): Confusion matrix visualization.
- [**`DLass1Q8.ipynb`**](https://github.com/SubikkshaS/da6401_assignment1/blob/main/DLass1Q8.ipynb): Cross-entropy vs. squared error loss comparison.
- [**`DLass1Q10.ipynb`**](https://github.com/SubikkshaS/da6401_assignment1/blob/main/DLass1Q10.ipynb): Applying insights from Fashion-MNIST to MNIST digit recognition.
- [**`Activations.ipynb`**](https://github.com/SubikkshaS/da6401_assignment1/blob/main/Activations.ipynb): Various activation function implementations.
- [**`Loss.ipynb`**](https://github.com/SubikkshaS/da6401_assignment1/blob/main/Loss.ipynb): Implementation of loss functions and comparisons.
- [**`Optimizers.ipynb`**](https://github.com/SubikkshaS/da6401_assignment1/blob/main/Optimizers.ipynb): Detailed optimizer implementations.
- [**`Helper.ipynb`**](https://github.com/SubikkshaS/da6401_assignment1/blob/main/Helper.ipynb): Utility and helper functions for neural network training.
- [**`Initializers.ipynb`**](https://github.com/SubikkshaS/da6401_assignment1/blob/main/Initializers.ipynb): Weight initialization strategies.
- [**`Layers.ipynb`**](https://github.com/SubikkshaS/da6401_assignment1/blob/main/Layers.ipynb): Implementation of neural network layers.
- [**`Network.ipynb`**](https://github.com/SubikkshaS/da6401_assignment1/blob/main/Network.ipynb): Neural network architecture definition.





