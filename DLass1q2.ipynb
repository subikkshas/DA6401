{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjaWE4hguA+vccZNbr79He",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subikkshas/DA6401/blob/main/DLass1q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2 (10 Marks)\n",
        "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.\n",
        "\n",
        "Your code should be flexible such that it is easy to change the number of hidden layers and the number of neurons in each hidden layer."
      ],
      "metadata": {
        "id": "QUWQzlXL5BGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "def one_hot(y, num_classes=10):\n",
        "    return np.eye(num_classes)[y]\n",
        "\n",
        "y_train, y_test = one_hot(y_train), one_hot(y_test)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_pred, y_true):\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-9), axis=1))\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size=784, hidden_layers=[128, 64], output_size=10, learning_rate=0.01):\n",
        "        self.layers = []\n",
        "        self.biases = []\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        layer_sizes = [input_size] + hidden_layers + [output_size]\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.layers.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01)\n",
        "            self.biases.append(np.zeros((1, layer_sizes[i+1])))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.a = [x]\n",
        "        self.z = []\n",
        "\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            z = np.dot(self.a[-1], self.layers[i]) + self.biases[i]\n",
        "            self.z.append(z)\n",
        "            self.a.append(relu(z))\n",
        "\n",
        "        final_z = np.dot(self.a[-1], self.layers[-1]) + self.biases[-1]\n",
        "        self.z.append(final_z)\n",
        "        self.a.append(softmax(final_z))\n",
        "\n",
        "        return self.a[-1]\n",
        "\n",
        "    def backward(self, y_true):\n",
        "        m = y_true.shape[0]\n",
        "        dz = self.a[-1] - y_true\n",
        "\n",
        "        for i in reversed(range(len(self.layers))):\n",
        "            dw = np.dot(self.a[i].T, dz) / m\n",
        "            db = np.sum(dz, axis=0, keepdims=True) / m\n",
        "\n",
        "            self.layers[i] -= self.learning_rate * dw\n",
        "            self.biases[i] -= self.learning_rate * db\n",
        "\n",
        "            if i > 0:\n",
        "                dz = np.dot(dz, self.layers[i].T) * relu_derivative(self.z[i-1])\n",
        "\n",
        "    def train(self, x_train, y_train, epochs=10, batch_size=32):\n",
        "        for epoch in range(epochs):\n",
        "            x_sample, y_sample = x_train[:1], y_train[:1]\n",
        "            y_pred = self.forward(x_sample)\n",
        "            print(\"Y Hat (Single Example):\", y_pred)\n",
        "            self.backward(y_sample)\n",
        "\n",
        "            y_pred_train = self.forward(x_train)\n",
        "            loss = cross_entropy_loss(y_pred_train, y_train)\n",
        "            accuracy = np.mean(np.argmax(y_pred_train, axis=1) == np.argmax(y_train, axis=1))\n",
        "\n",
        "            print(f\"Epoch {epoch+1}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "nn = NeuralNetwork(hidden_layers=[128, 64], learning_rate=0.01)\n",
        "nn.train(x_train, y_train, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VZfowWC5FNi",
        "outputId": "60f2d8f8-a6d2-4a80-8903-e7c404e1faba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y Hat (Single Example): [[0.10000889 0.10005186 0.10008474 0.09999062 0.09990906 0.09996893\n",
            "  0.10005077 0.1000124  0.09994895 0.09997379]]\n",
            "Epoch 1: Loss = 2.3026, Accuracy = 0.1000\n",
            "Y Hat (Single Example): [[0.09990659 0.09994996 0.09998358 0.09988935 0.09980724 0.09986501\n",
            "  0.09994992 0.0999122  0.09984952 0.10088664]]\n",
            "Epoch 2: Loss = 2.3027, Accuracy = 0.1000\n",
            "Y Hat (Single Example): [[0.09980362 0.09984732 0.09988182 0.09978729 0.0997046  0.09976046\n",
            "  0.09984857 0.09981126 0.0997493  0.10180576]]\n",
            "Epoch 3: Loss = 2.3027, Accuracy = 0.1000\n",
            "Y Hat (Single Example): [[0.09969994 0.09974399 0.09977937 0.0996845  0.09960121 0.09965518\n",
            "  0.09974651 0.09970961 0.09964833 0.10273134]]\n",
            "Epoch 4: Loss = 2.3027, Accuracy = 0.1000\n",
            "Y Hat (Single Example): [[0.09959575 0.09964069 0.09967668 0.0995808  0.09949703 0.09954947\n",
            "  0.09964367 0.09960709 0.0995463  0.10366252]]\n",
            "Epoch 5: Loss = 2.3027, Accuracy = 0.1000\n",
            "Y Hat (Single Example): [[0.09949089 0.09953672 0.09957321 0.09947674 0.09939211 0.09944302\n",
            "  0.09953999 0.09950383 0.09944328 0.10460021]]\n",
            "Epoch 6: Loss = 2.3028, Accuracy = 0.1000\n",
            "Y Hat (Single Example): [[0.0993854  0.09943211 0.09946934 0.09937143 0.0992869  0.09933625\n",
            "  0.09943529 0.09940008 0.09933954 0.10554367]]\n",
            "Epoch 7: Loss = 2.3028, Accuracy = 0.1000\n",
            "Y Hat (Single Example): [[0.09927911 0.09932688 0.09936501 0.09926528 0.09918114 0.09922891\n",
            "  0.09932942 0.0992958  0.09923492 0.10649351]]\n",
            "Epoch 8: Loss = 2.3029, Accuracy = 0.1000\n",
            "Y Hat (Single Example): [[0.09917208 0.0992208  0.09926007 0.09915848 0.09907468 0.09912092\n",
            "  0.09922276 0.09919091 0.09912956 0.10744973]]\n",
            "Epoch 9: Loss = 2.3030, Accuracy = 0.1000\n",
            "Y Hat (Single Example): [[0.09906431 0.09911381 0.0991543  0.09905115 0.09896747 0.09901254\n",
            "  0.09911525 0.09908543 0.09902348 0.10841227]]\n",
            "Epoch 10: Loss = 2.3031, Accuracy = 0.1000\n"
          ]
        }
      ]
    }
  ]
}