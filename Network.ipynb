{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPntHSvQ9Wwuj7ChDPQCBY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subikkshas/DA6401/blob/main/Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a9f2PP_KlsKl"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import wandb\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from copy import deepcopy\n",
        "from activations import Sigmoid, Tanh, ReLU, Softmax\n",
        "from layers import Input, Dense\n",
        "from optimizers import SGD, Momentum, Nesterov, RMSProp, Adam, Nadam\n",
        "from layers import Input, Dense\n",
        "from loss import CrossEntropy, SquaredError\n",
        "from helper import OneHotEncoder\n",
        "\n",
        "map_optimizers = {\"SGD\":SGD(), \"Momentum\":Momentum(), \"Nesterov\":Nesterov(), \"RMSProp\":RMSProp(), \"Adam\":Adam(), \"Nadam\":Nadam()}\n",
        "map_losses = {\"SquaredError\":SquaredError(), \"CrossEntropy\":CrossEntropy()}\n",
        "################################################\n",
        "#         Network\n",
        "################################################\n",
        "class NeuralNetwork():\n",
        "    def __init__(self, layers, batch_size, optimizer, initialization, epochs, t, loss, X_val=None, t_val=None, use_wandb=False, optim_params=None):\n",
        "        self.layers = layers\n",
        "        self.batch_size = batch_size\n",
        "        self.initialization = initialization\n",
        "        self.epochs = epochs\n",
        "        self.optimizer = optimizer\n",
        "        self.t = t\n",
        "        self.num_batches = math.ceil(self.t.shape[1]/batch_size)\n",
        "        self.loss_type = loss\n",
        "        self.loss = map_losses[loss]\n",
        "        self.use_wandb = use_wandb\n",
        "        if t_val is not None:\n",
        "            self.X_val = X_val\n",
        "            self.layers[0].a_val = X_val\n",
        "            self.t_val = t_val\n",
        "        self.param_init(optimizer, optim_params)\n",
        "\n",
        "    def param_init(self, optimizer, optim_params):\n",
        "        size_prev = self.layers[0].size\n",
        "        for layer in self.layers[1:]:\n",
        "            # layer.W_size = (layer.size, size_prev+1)\n",
        "            layer.W_size = (layer.size, size_prev)\n",
        "            size_prev = layer.size\n",
        "            layer.W_optimizer = deepcopy(map_optimizers[optimizer])\n",
        "            layer.b_optimizer = deepcopy(map_optimizers[optimizer])\n",
        "            # Code to set params\n",
        "            if optim_params:\n",
        "                layer.W_optimizer.set_params(optim_params)\n",
        "                layer.b_optimizer.set_params(optim_params)\n",
        "\n",
        "        if self.initialization == \"RandomNormal\":\n",
        "            for layer in self.layers[1:]:\n",
        "                layer.W = np.random.normal(loc=0, scale=1.0, size = layer.W_size)\n",
        "                layer.b = np.zeros((layer.W_size[0], 1))\n",
        "\n",
        "        elif self.initialization == \"XavierUniform\":\n",
        "            for layer in self.layers[1:]:\n",
        "                initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05)#, seed=42)\n",
        "                layer.W = np.array(initializer(shape=layer.W_size))\n",
        "                layer.b = np.zeros((layer.W_size[0], 1))\n",
        "\n",
        "        elif self.initialization == \"Test\":\n",
        "            for layer in self.layers[1:]:\n",
        "                layer.W = np.ones(layer.W_size)*0.5\n",
        "                layer.b = np.zeros((layer.W_size[0], 1))\n",
        "\n",
        "\n",
        "    def forward_propogation(self):\n",
        "        for i in range(1, len(self.layers)):\n",
        "            # print(\"Layer:\", i, self.layers[i].W.shape)\n",
        "            # Pre-activation\n",
        "            self.layers[i].h = self.layers[i].W @ self.layers[i-1].a - self.layers[i].b\n",
        "            # Activation\n",
        "            self.layers[i].a = self.layers[i].activation.value(self.layers[i].h)\n",
        "            # Validation\n",
        "            self.layers[i].h_val = self.layers[i].W @ self.layers[i-1].a_val - self.layers[i].b\n",
        "            self.layers[i].a_val = self.layers[i].activation.value(self.layers[i].h_val)\n",
        "\n",
        "        if self.loss_type == \"CrossEntropy\":\n",
        "            # Final sofmax activation\n",
        "            self.layers[-1].y = Softmax().value(self.layers[-1].a)\n",
        "            self.layers[-1].y_val = Softmax().value(self.layers[-1].a_val)\n",
        "        else:\n",
        "            self.layers[-1].y = self.layers[-1].a\n",
        "            self.layers[-1].y_val = self.layers[-1].a_val\n",
        "\n",
        "    def check_test(self, X_test, t_test):\n",
        "        self.layers[0].a_test = X_test\n",
        "        for i in range(1, len(self.layers)):\n",
        "            self.layers[i].h_test = self.layers[i].W @ self.layers[i-1].a_test - self.layers[i].b\n",
        "            self.layers[i].a_test = self.layers[i].activation.value(self.layers[i].h_test)\n",
        "\n",
        "        if self.loss==\"CrossEntropy\":\n",
        "            self.layers[-1].y_test = Softmax().value(self.layers[-1].a_test)\n",
        "        else:\n",
        "            self.layers[-1].y_test = self.layers[-1].a_test\n",
        "\n",
        "        loss_test = self.loss.calc_loss(t_test, self.layers[-1].y_test)\n",
        "\n",
        "        encoder = OneHotEncoder()\n",
        "        y_tmp = encoder.inverse_transform(self.layers[-1].y_test)\n",
        "        t_tmp = encoder.inverse_transform(t_test)\n",
        "        acc_test = np.sum(y_tmp==t_tmp)\n",
        "        return acc_test, loss_test, y_tmp\n",
        "\n",
        "\n",
        "    def backward_propogation(self):\n",
        "        # Initialize variables neesed to keep track of loss\n",
        "        self.eta_hist = []\n",
        "        self.loss_hist = []\n",
        "        self.accuracy_hist = []\n",
        "        self.loss_hist_val = []\n",
        "        self.accuracy_hist_val = []\n",
        "        self.loss = SquaredError()\n",
        "        flag = 0\n",
        "\n",
        "        # Perform Backprop\n",
        "        # for _ in range(self.epochs):\n",
        "        for ep in tqdm(range(self.epochs)):\n",
        "            self.eta_hist.append(self.layers[-1].W_optimizer.eta)\n",
        "            self.loss_hist.append(self.loss.calc_loss(self.t, self.layers[-1].y))\n",
        "            train_acc, val_acc = self.get_accuracy(validation=True)\n",
        "            self.accuracy_hist.append(train_acc)\n",
        "            self.loss_hist_val.append(self.loss.calc_loss(self.t_val, self.layers[-1].y_val))\n",
        "            self.accuracy_hist_val.append(val_acc)\n",
        "\n",
        "            if self.use_wandb:\n",
        "                wandb.log({\n",
        "                            \"step\": ep, \\\n",
        "                            \"loss:\": self.loss_hist[-1]/self.t.shape[1], \\\n",
        "                            \"accuracy\": self.accuracy_hist[-1]/self.t.shape[1], \\\n",
        "                            \"val_loss\": self.loss_hist_val[-1]/self.t_val.shape[1], \\\n",
        "                            \"val_accuracy\": self.accuracy_hist_val[-1]/self.t_val.shape[1]\n",
        "                        })\n",
        "\n",
        "            for batch in range(self.num_batches):\n",
        "                # print(\"\\n\", \"=\"*50)\n",
        "                # print(\"Batch:\", batch)\n",
        "                # X_batch = self.layers[0].input[batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "                t_batch = self.t[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "                y_batch = self.layers[-1].y[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "                self.y_batch = y_batch\n",
        "                self.t_batch = t_batch\n",
        "\n",
        "                # Calculate Loss, grad wrt y and softmax for last layer\n",
        "                # print(\"t:\\n\", self.t)\n",
        "                # print(\"y:\\n\", self.layers[-1].y)\n",
        "                # print(self.loss_hist[-1])\n",
        "\n",
        "                try:\n",
        "                    if self.loss_hist[-1] > self.loss_hist[-2]:\n",
        "                        for layer in self.layers[1:]:\n",
        "                            layer.W_optimizer.set_params({\"eta\":self.optimizer.eta/2})\n",
        "                            layer.b_optimizer.set_params({\"eta\":self.optimizer.eta/2})\n",
        "                        flag = 1\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                if flag == 1:\n",
        "                    break\n",
        "\n",
        "                # self.layers[-1].cross_grad = self.loss.diff()\n",
        "                self.layers[-1].a_grad = self.loss.diff(self.t_batch, self.y_batch)\n",
        "                self.layers[-1].h_grad = self.layers[-1].a_grad * self.layers[-1].activation.diff(self.layers[-1].h[:, batch*self.batch_size:(batch+1)*self.batch_size])\n",
        "\n",
        "                self.layers[-1].W_grad = self.layers[-1].h_grad @ self.layers[-2].a[:, batch*self.batch_size:(batch+1)*self.batch_size].T\n",
        "                self.layers[-1].W_update = self.layers[-1].W_optimizer.get_update(self.layers[-1].W_grad)\n",
        "\n",
        "                self.layers[-1].b_grad = -np.sum(self.layers[-1].h_grad, axis=1).reshape(-1,1)\n",
        "                self.layers[-1].b_update = self.layers[-1].b_optimizer.get_update(self.layers[-1].b_grad)\n",
        "\n",
        "                # print(\"Last Layer\")\n",
        "                # print(\"a_grad shape:\", self.layers[-1].a_grad.shape)\n",
        "                # print(\"h_grad shape:\", self.layers[-1].h_grad.shape)\n",
        "                # print(\"W_grad shape:\", self.layers[-1].W_grad.shape)\n",
        "                # print(\"W_update shape:\", self.layers[-1].W_update.shape)\n",
        "                # print(\"W_shape:\", self.layers[-1].W.shape)\n",
        "                # print(\"a_grad:\\n\", self.layers[-1].a_grad)\n",
        "                # print(\"h_grad:\\n\", self.layers[-1].h_grad)\n",
        "                # print(\"W_grad:\\n\", self.layers[-1].W_grad)\n",
        "\n",
        "                assert self.layers[-1].W_update.shape == self.layers[-1].W.shape, \"Sizes don't match\"\n",
        "\n",
        "\n",
        "                # Backpropogation for the remaining layers\n",
        "                for i in range(len(self.layers[:-2]), 0, -1):\n",
        "                    self.layers[i].a_grad = self.layers[i+1].W.T @ self.layers[i+1].h_grad\n",
        "                    self.layers[i].h_grad = self.layers[i].a_grad * self.layers[i].activation.diff(self.layers[i].h[:, batch*self.batch_size:(batch+1)*self.batch_size])\n",
        "                    # print(\"Layer -\", i)\n",
        "                    # print(\"a_grad shape:\", self.layers[i].a_grad.shape)\n",
        "                    # print(\"h_grad shape:\", self.layers[i].h_grad.shape)\n",
        "\n",
        "                    # print(\"Layer -\", i)\n",
        "                    # print(\"a_grad:\", self.layers[i].a_grad)\n",
        "                    # print(\"h_grad:\", self.layers[i].h_grad)\n",
        "\n",
        "                    self.layers[i].b_grad = -np.sum(self.layers[i].h_grad, axis=1).reshape(-1,1)\n",
        "                    self.layers[i].W_grad = self.layers[i].h_grad @ self.layers[i-1].a[:, batch*self.batch_size:(batch+1)*self.batch_size].T\n",
        "\n",
        "                    # print(\"W_grad shape:\", self.layers[i].W_grad.shape)\n",
        "                    # print(\"W_grad:\", self.layers[i].W_grad)\n",
        "                    # print()\n",
        "                    self.layers[i].W_update = self.layers[i].W_optimizer.get_update(self.layers[i].W_grad)\n",
        "                    self.layers[i].b_update = self.layers[i].b_optimizer.get_update(self.layers[i].b_grad)\n",
        "                    # self.layers[i].b_update = self.layers[i].b_optimizer.get_update(self.layers[i].b_grad)\n",
        "\n",
        "                # Update the weights\n",
        "                for _, layer in enumerate(self.layers[1:]):\n",
        "                    layer.W = layer.W - layer.W_update\n",
        "                    layer.b = layer.b - layer.b_update\n",
        "                    # print(\"Layer -\", idx)\n",
        "                    # print(\"W:\\n\", layer.W)\n",
        "                    # print(\"h:\\n\", layer.h)\n",
        "\n",
        "                    # layer.b = layer.b - self.b_update\n",
        "                # print(\"Y:\\n\", self.layers[-1].y)\n",
        "                self.forward_propogation()\n",
        "\n",
        "            if flag == 1:\n",
        "                break\n",
        "\n",
        "    def describe(self):\n",
        "        print(\"Model with the following layers:\")\n",
        "        for i in self.layers:\n",
        "            print(i)\n",
        "        print(\"Loss:\", self.loss)\n",
        "        print(\"Epochs:\", self.epochs)\n",
        "        print(\"Batch Size:\", self.batch_size)\n",
        "        print(\"Optimizer:\", self.optimizer)\n",
        "        print(\"Initialization:\", self.initialization)\n",
        "\n",
        "    def get_accuracy(self, validation=False, print_vals=False):\n",
        "        encoder = OneHotEncoder()\n",
        "        t_train = encoder.inverse_transform(self.t)\n",
        "        y_train = encoder.inverse_transform(self.layers[-1].y)\n",
        "        acc_train = np.sum(t_train==y_train)\n",
        "        if print_vals:\n",
        "            print(\"Train Accuracy:\", acc_train)\n",
        "\n",
        "        if validation:\n",
        "            t_val = encoder.inverse_transform(self.t_val)\n",
        "            y_val = encoder.inverse_transform(self.layers[-1].y_val)\n",
        "            acc_val = np.sum(t_val==y_val)\n",
        "            if print_vals:\n",
        "                print(\"Validation Accuracy:\", acc_val)\n",
        "            return acc_train, acc_val\n",
        "        return acc_train"
      ]
    }
  ]
}