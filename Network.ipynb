{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBuJZGbNcqJv4Fq3xNCcL5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subikkshas/DA6401/blob/main/Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a9f2PP_KlsKl"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import wandb\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from copy import deepcopy\n",
        "from activations import Sigmoid, Tanh, ReLU, Softmax\n",
        "from layers import Input, Dense\n",
        "from optimizers import SGD, Momentum, Nesterov, RMSProp, Adam, Nadam\n",
        "from loss import CrossEntropy, SquaredError\n",
        "from helper import OneHotEncoder\n",
        "\n",
        "# Mapping available optimizers and loss functions\n",
        "optimizer_mapping = {\n",
        "    \"SGD\": SGD(),\n",
        "    \"Momentum\": Momentum(),\n",
        "    \"Nesterov\": Nesterov(),\n",
        "    \"RMSProp\": RMSProp(),\n",
        "    \"Adam\": Adam(),\n",
        "    \"Nadam\": Nadam()\n",
        "}\n",
        "\n",
        "loss_mapping = {\n",
        "    \"SquaredError\": SquaredError(),\n",
        "    \"CrossEntropy\": CrossEntropy()\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Neural Network\n",
        "\n",
        "class NeuralNetwork:\n",
        "    \"\"\"Implements a feedforward neural network with backpropagation.\"\"\"\n",
        "\n",
        "    def __init__(self, layers, batch_size, optimizer, initialization, epochs, target, loss,\n",
        "                 X_val=None, target_val=None, use_wandb=False, optim_params=None):\n",
        "        \"\"\"\n",
        "        Initializes the neural network.\n",
        "\n",
        "        Args:\n",
        "            layers (list): List of layers in the network.\n",
        "            batch_size (int): Number of samples per batch.\n",
        "            optimizer (str): Optimization algorithm.\n",
        "            initialization (str): Weight initialization method.\n",
        "            epochs (int): Number of training epochs.\n",
        "            target (np.ndarray): Training target labels.\n",
        "            loss (str): Loss function name.\n",
        "            X_val (np.ndarray, optional): Validation dataset.\n",
        "            target_val (np.ndarray, optional): Validation target labels.\n",
        "            use_wandb (bool, optional): Whether to log training on Weights & Biases.\n",
        "            optim_params (dict, optional): Parameters for optimizers.\n",
        "        \"\"\"\n",
        "        self.layers = layers\n",
        "        self.batch_size = batch_size\n",
        "        self.initialization = initialization\n",
        "        self.epochs = epochs\n",
        "        self.optimizer = optimizer\n",
        "        self.target = target\n",
        "        self.num_batches = math.ceil(self.target.shape[1] / batch_size)\n",
        "        self.loss_type = loss\n",
        "        self.loss_fn = loss_mapping[loss]\n",
        "        self.use_wandb = use_wandb\n",
        "\n",
        "        if target_val is not None:\n",
        "            self.X_val = X_val\n",
        "            self.layers[0].a_val = X_val\n",
        "            self.target_val = target_val\n",
        "\n",
        "        self.initialize_parameters(optimizer, optim_params)\n",
        "\n",
        "    def initialize_parameters(self, optimizer, optim_params):\n",
        "        \"\"\"Initializes weights and biases for each layer.\"\"\"\n",
        "        previous_size = self.layers[0].size\n",
        "        for layer in self.layers[1:]:\n",
        "            layer.W_size = (layer.size, previous_size)\n",
        "            previous_size = layer.size\n",
        "            layer.W_optimizer = deepcopy(optimizer_mapping[optimizer])\n",
        "            layer.b_optimizer = deepcopy(optimizer_mapping[optimizer])\n",
        "\n",
        "            # Assign optimizer parameters if provided\n",
        "            if optim_params:\n",
        "                layer.W_optimizer.configure_params(optim_params)\n",
        "                layer.b_optimizer.configure_params(optim_params)\n",
        "\n",
        "        if self.initialization == \"RandomNormal\":\n",
        "            for layer in self.layers[1:]:\n",
        "                layer.W = np.random.normal(loc=0, scale=1.0, size=layer.W_size)\n",
        "                layer.b = np.zeros((layer.W_size[0], 1))\n",
        "\n",
        "        elif self.initialization == \"XavierUniform\":\n",
        "            for layer in self.layers[1:]:\n",
        "                initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05)\n",
        "                layer.W = np.array(initializer(shape=layer.W_size))\n",
        "                layer.b = np.zeros((layer.W_size[0], 1))\n",
        "\n",
        "        elif self.initialization == \"Test\":\n",
        "            for layer in self.layers[1:]:\n",
        "                layer.W = np.ones(layer.W_size) * 0.5\n",
        "                layer.b = np.zeros((layer.W_size[0], 1))\n",
        "\n",
        "    def forward_pass(self):\n",
        "        \"\"\"Performs forward propagation through the network.\"\"\"\n",
        "        for i in range(1, len(self.layers)):\n",
        "            self.layers[i].h = self.layers[i].W @ self.layers[i - 1].a - self.layers[i].b\n",
        "            self.layers[i].a = self.layers[i].activation.compute(self.layers[i].h)\n",
        "\n",
        "            if hasattr(self, \"X_val\"):\n",
        "                self.layers[i].h_val = self.layers[i].W @ self.layers[i - 1].a_val - self.layers[i].b\n",
        "                self.layers[i].a_val = self.layers[i].activation.compute(self.layers[i].h_val)\n",
        "\n",
        "        if self.loss_type == \"CrossEntropy\":\n",
        "            self.layers[-1].y = Softmax().compute(self.layers[-1].a)\n",
        "            self.layers[-1].y_val = Softmax().compute(self.layers[-1].a_val)\n",
        "        else:\n",
        "            self.layers[-1].y = self.layers[-1].a\n",
        "            self.layers[-1].y_val = self.layers[-1].a_val\n",
        "\n",
        "    def compute_accuracy(self, validation=False, verbose=False):\n",
        "        \"\"\"Computes the accuracy of the model.\"\"\"\n",
        "        encoder = OneHotEncoder()\n",
        "        y_train = encoder.inverse_transform(self.layers[-1].y)\n",
        "        t_train = encoder.inverse_transform(self.target)\n",
        "        acc_train = np.sum(y_train == t_train)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Training Accuracy:\", acc_train)\n",
        "\n",
        "        if validation:\n",
        "            y_val = encoder.inverse_transform(self.layers[-1].y_val)\n",
        "            t_val = encoder.inverse_transform(self.target_val)\n",
        "            acc_val = np.sum(y_val == t_val)\n",
        "\n",
        "            if verbose:\n",
        "                print(\"Validation Accuracy:\", acc_val)\n",
        "            return acc_train, acc_val\n",
        "        return acc_train\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Trains the neural network using backpropagation.\"\"\"\n",
        "        self.learning_rate_history = []\n",
        "        self.training_loss_history = []\n",
        "        self.training_accuracy_history = []\n",
        "        self.validation_loss_history = []\n",
        "        self.validation_accuracy_history = []\n",
        "\n",
        "        loss_fn = SquaredError()\n",
        "\n",
        "        for epoch in tqdm(range(self.epochs), desc=\"Training Progress\"):\n",
        "            self.learning_rate_history.append(self.layers[-1].W_optimizer.learning_rate)\n",
        "            self.training_loss_history.append(loss_fn.compute_loss(self.target, self.layers[-1].y))\n",
        "            train_acc, val_acc = self.compute_accuracy(validation=True)\n",
        "            self.training_accuracy_history.append(train_acc)\n",
        "            self.validation_loss_history.append(loss_fn.compute_loss(self.target_val, self.layers[-1].y_val))\n",
        "            self.validation_accuracy_history.append(val_acc)\n",
        "\n",
        "            if self.use_wandb:\n",
        "                wandb.log({\n",
        "                    \"epoch\": epoch,\n",
        "                    \"training_loss\": self.training_loss_history[-1] / self.target.shape[1],\n",
        "                    \"training_accuracy\": self.training_accuracy_history[-1] / self.target.shape[1],\n",
        "                    \"validation_loss\": self.validation_loss_history[-1] / self.target_val.shape[1],\n",
        "                    \"validation_accuracy\": self.validation_accuracy_history[-1] / self.target_val.shape[1]\n",
        "                })\n",
        "\n",
        "            for batch in range(self.num_batches):\n",
        "                t_batch = self.target[:, batch * self.batch_size:(batch + 1) * self.batch_size]\n",
        "                y_batch = self.layers[-1].y[:, batch * self.batch_size:(batch + 1) * self.batch_size]\n",
        "\n",
        "                if loss_fn.compute_loss(t_batch, y_batch) > self.training_loss_history[-1]:\n",
        "                    for layer in self.layers[1:]:\n",
        "                        layer.W_optimizer.configure_params({\"learning_rate\": self.optimizer.learning_rate / 2})\n",
        "                        layer.b_optimizer.configure_params({\"learning_rate\": self.optimizer.learning_rate / 2})\n",
        "                    break\n",
        "\n",
        "                self.layers[-1].a_grad = loss_fn.compute_gradient(t_batch, y_batch)\n",
        "                self.layers[-1].h_grad = self.layers[-1].a_grad * self.layers[-1].activation.derivative(self.layers[-1].h)\n",
        "\n",
        "                for i in range(len(self.layers) - 2, 0, -1):\n",
        "                    self.layers[i].a_grad = self.layers[i + 1].W.T @ self.layers[i + 1].h_grad\n",
        "                    self.layers[i].h_grad = self.layers[i].a_grad * self.layers[i].activation.derivative(self.layers[i].h)\n",
        "\n",
        "                for layer in self.layers[1:]:\n",
        "                    layer.W -= layer.W_optimizer.compute_update(layer.W_grad)\n",
        "                    layer.b -= layer.b_optimizer.compute_update(layer.b_grad)\n",
        "\n",
        "            self.forward_pass()\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"Displays the model summary.\"\"\"\n",
        "        print(\"Model Architecture:\")\n",
        "        for layer in self.layers:\n",
        "            print(layer)\n",
        "        print(\"Loss Function:\", self.loss_fn)\n",
        "        print(\"Epochs:\", self.epochs)\n",
        "        print(\"Batch Size:\", self.batch_size)\n",
        "        print(\"Optimizer:\", self.optimizer)\n",
        "        print(\"Initialization Method:\", self.initialization)\n"
      ]
    }
  ]
}