{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkw2teRFvmonObxJ2HdABh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subikkshas/DA6401/blob/main/DLass1Q10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n93bppNDiwvw",
        "outputId": "e6a5c414-bb9e-45ff-de9d-a9af976ace03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration 1:\n",
            "Epoch 1/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.8263 - loss: 0.6186 - val_accuracy: 0.9617 - val_loss: 0.1426\n",
            "Epoch 2/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9537 - loss: 0.1562 - val_accuracy: 0.9700 - val_loss: 0.1081\n",
            "Epoch 3/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9687 - loss: 0.1057 - val_accuracy: 0.9708 - val_loss: 0.1027\n",
            "Epoch 4/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9772 - loss: 0.0769 - val_accuracy: 0.9723 - val_loss: 0.0935\n",
            "Epoch 5/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9831 - loss: 0.0597 - val_accuracy: 0.9760 - val_loss: 0.0851\n",
            "Epoch 6/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9848 - loss: 0.0498 - val_accuracy: 0.9772 - val_loss: 0.0801\n",
            "Epoch 7/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9888 - loss: 0.0387 - val_accuracy: 0.9772 - val_loss: 0.0832\n",
            "Epoch 8/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9895 - loss: 0.0327 - val_accuracy: 0.9748 - val_loss: 0.0851\n",
            "Epoch 9/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9902 - loss: 0.0310 - val_accuracy: 0.9795 - val_loss: 0.0820\n",
            "Epoch 10/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9930 - loss: 0.0229 - val_accuracy: 0.9813 - val_loss: 0.0791\n",
            "Test Accuracy: 0.9778\n",
            "Configuration 2:\n",
            "Epoch 1/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8512 - loss: 0.5165 - val_accuracy: 0.9653 - val_loss: 0.1209\n",
            "Epoch 2/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9611 - loss: 0.1300 - val_accuracy: 0.9747 - val_loss: 0.0912\n",
            "Epoch 3/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9737 - loss: 0.0860 - val_accuracy: 0.9745 - val_loss: 0.0809\n",
            "Epoch 4/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9803 - loss: 0.0659 - val_accuracy: 0.9773 - val_loss: 0.0772\n",
            "Epoch 5/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9836 - loss: 0.0517 - val_accuracy: 0.9775 - val_loss: 0.0774\n",
            "Epoch 6/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9884 - loss: 0.0379 - val_accuracy: 0.9765 - val_loss: 0.0834\n",
            "Epoch 7/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9904 - loss: 0.0307 - val_accuracy: 0.9753 - val_loss: 0.0854\n",
            "Epoch 8/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9904 - loss: 0.0285 - val_accuracy: 0.9768 - val_loss: 0.0858\n",
            "Epoch 9/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9936 - loss: 0.0209 - val_accuracy: 0.9783 - val_loss: 0.0867\n",
            "Epoch 10/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9941 - loss: 0.0180 - val_accuracy: 0.9762 - val_loss: 0.0947\n",
            "Test Accuracy: 0.9747\n",
            "Configuration 3:\n",
            "Epoch 1/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.7990 - loss: 0.7100 - val_accuracy: 0.9625 - val_loss: 0.1431\n",
            "Epoch 2/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9527 - loss: 0.1663 - val_accuracy: 0.9708 - val_loss: 0.1034\n",
            "Epoch 3/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9698 - loss: 0.1074 - val_accuracy: 0.9748 - val_loss: 0.0888\n",
            "Epoch 4/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9785 - loss: 0.0739 - val_accuracy: 0.9765 - val_loss: 0.0781\n",
            "Epoch 5/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.9831 - loss: 0.0599 - val_accuracy: 0.9780 - val_loss: 0.0799\n",
            "Epoch 6/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9867 - loss: 0.0469 - val_accuracy: 0.9780 - val_loss: 0.0770\n",
            "Epoch 7/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9908 - loss: 0.0331 - val_accuracy: 0.9792 - val_loss: 0.0737\n",
            "Epoch 8/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9921 - loss: 0.0271 - val_accuracy: 0.9790 - val_loss: 0.0743\n",
            "Epoch 9/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9938 - loss: 0.0224 - val_accuracy: 0.9795 - val_loss: 0.0777\n",
            "Epoch 10/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.9960 - loss: 0.0165 - val_accuracy: 0.9822 - val_loss: 0.0674\n",
            "Test Accuracy: 0.9805\n",
            "Configuration 1: Adam optimizer, learning rate = 0.001, batch size = 128, 2-layer network (128, 64), ReLU \n",
            "Test Accuracy: 0.9778\n",
            "Configuration 2: Adam optimizer, learning rate = 0.001, batch size = 64, 2-layer network (128, 64), ReLU \n",
            "Test Accuracy: 0.9747\n",
            "Configuration 3: Adam optimizer, learning rate = 0.0005, batch size = 128, 2-layer network (256, 128), ReLU \n",
            "Test Accuracy: 0.9805\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Define a function to build and train the model\n",
        "def train_mnist_model(learning_rate, batch_size, hidden_units, activation, optimizer):\n",
        "\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),  # Flatten the 28x28 input images\n",
        "        Dense(hidden_units[0], activation=activation),  # First hidden layer\n",
        "        Dense(hidden_units[1], activation=activation),  # Second hidden layer\n",
        "        Dense(10, activation='softmax')  # Output layer\n",
        "    ])\n",
        "\n",
        "\n",
        "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=10, batch_size=batch_size, validation_split=0.1, verbose=1)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    return test_accuracy\n",
        "\n",
        "# Configuration 1: Adam optimizer, learning rate = 0.001, batch size = 128, 2-layer network (128, 64), ReLU\n",
        "print(\"Configuration 1:\")\n",
        "accuracy_1 = train_mnist_model(learning_rate=0.001, batch_size=128, hidden_units=[128, 64], activation='relu', optimizer=Adam)\n",
        "\n",
        "# Configuration 2: Adam optimizer, learning rate = 0.001, batch size = 64, 2-layer network (128, 64), ReLU\n",
        "print(\"Configuration 2:\")\n",
        "accuracy_2 = train_mnist_model(learning_rate=0.001, batch_size=64, hidden_units=[128, 64], activation='relu', optimizer=Adam)\n",
        "\n",
        "# Configuration 3: Adam optimizer, learning rate = 0.0005, batch size = 128, 2-layer network (256, 128), ReLU\n",
        "print(\"Configuration 3:\")\n",
        "accuracy_3 = train_mnist_model(learning_rate=0.0005, batch_size=128, hidden_units=[256, 128], activation='relu', optimizer=Adam)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Configuration 1: Adam optimizer, learning rate = 0.001, batch size = 128, 2-layer network (128, 64), ReLU \\nTest Accuracy: {accuracy_1:.4f}\")\n",
        "print(f\"Configuration 2: Adam optimizer, learning rate = 0.001, batch size = 64, 2-layer network (128, 64), ReLU \\nTest Accuracy: {accuracy_2:.4f}\")\n",
        "print(f\"Configuration 3: Adam optimizer, learning rate = 0.0005, batch size = 128, 2-layer network (256, 128), ReLU \\nTest Accuracy: {accuracy_3:.4f}\")"
      ]
    }
  ]
}