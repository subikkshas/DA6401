{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeDStPhEXQGXN9gRRQhIeX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subikkshas/DA6401/blob/main/Loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BmPoWzuni_wj"
      },
      "outputs": [],
      "source": [
        "# Loss Functions\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CrossEntropy:\n",
        "    \"\"\"Computes the Cross-Entropy loss and its gradient.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def compute_loss(self, target, prediction):\n",
        "        \"\"\"\n",
        "        Computes the Cross-Entropy loss.\n",
        "\n",
        "        Args:\n",
        "        - target (np.ndarray): One-hot encoded target values.\n",
        "        - prediction (np.ndarray): Predicted probabilities.\n",
        "\n",
        "        Returns:\n",
        "        - float: Cross-entropy loss value.\n",
        "        \"\"\"\n",
        "        self.target = target\n",
        "        self.prediction = np.clip(prediction, 1e-10, 1.0)  # Prevent log(0) issues\n",
        "        return -np.sum(self.target * np.log(self.prediction))\n",
        "\n",
        "    def compute_gradient(self):\n",
        "        \"\"\"\n",
        "        Computes the gradient of Cross-Entropy loss with respect to predictions.\n",
        "\n",
        "        Returns:\n",
        "        - np.ndarray: Gradient matrix.\n",
        "        \"\"\"\n",
        "        return -self.target / self.prediction\n",
        "\n",
        "\n",
        "class SquaredError:\n",
        "    \"\"\"Computes the Mean Squared Error (MSE) loss and its gradient.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def compute_loss(self, target, prediction):\n",
        "        \"\"\"\n",
        "        Computes the Squared Error loss.\n",
        "\n",
        "        Args:\n",
        "        - target (np.ndarray): Ground truth values.\n",
        "        - prediction (np.ndarray): Predicted values.\n",
        "\n",
        "        Returns:\n",
        "        - float: Squared error loss value.\n",
        "        \"\"\"\n",
        "        return np.sum((target - prediction) ** 2)\n",
        "\n",
        "    def compute_gradient(self, target_batch, prediction_batch):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the Squared Error loss.\n",
        "\n",
        "        Args:\n",
        "        - target_batch (np.ndarray): Batch of ground truth values.\n",
        "        - prediction_batch (np.ndarray): Batch of predicted values.\n",
        "\n",
        "        Returns:\n",
        "        - np.ndarray: Gradient matrix.\n",
        "        \"\"\"\n",
        "        return -(target_batch - prediction_batch)\n"
      ]
    }
  ]
}