{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgQY2ZI9NHQ7asYTQBPgnK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subikkshas/DA6401/blob/main/DLass1q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -q"
      ],
      "metadata": {
        "id": "HHMyxeENEfwX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import wandb"
      ],
      "metadata": {
        "id": "ibRH0HqLEBSV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xSE4Jojx7vkC"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-8)) / y_true.shape[0]\n",
        "\n",
        "def cross_entropy_derivative(y_true, y_pred):\n",
        "    return y_pred - y_true\n",
        "\n",
        "class Optimizer:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "    def update(self, weights, gradients):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class SGD(Optimizer):\n",
        "    def update(self, weights, gradients):\n",
        "        return weights - self.learning_rate * gradients\n",
        "\n",
        "class Momentum(Optimizer):\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        super().__init__(learning_rate)\n",
        "        self.momentum = momentum\n",
        "        self.velocity = 0\n",
        "    def update(self, weights, gradients):\n",
        "        self.velocity = self.momentum * self.velocity - self.learning_rate * gradients\n",
        "        return weights + self.velocity\n",
        "\n",
        "class NAG(Momentum):\n",
        "    def update(self, weights, gradients):\n",
        "        lookahead = weights + self.momentum * self.velocity\n",
        "        self.velocity = self.momentum * self.velocity - self.learning_rate * gradients\n",
        "        return lookahead + self.velocity\n",
        "\n",
        "class RMSprop(Optimizer):\n",
        "    def __init__(self, learning_rate=0.01, beta=0.9, epsilon=1e-8):\n",
        "        super().__init__(learning_rate)\n",
        "        self.beta = beta\n",
        "        self.epsilon = epsilon\n",
        "        self.squared_gradients = 0\n",
        "    def update(self, weights, gradients):\n",
        "        self.squared_gradients = self.beta * self.squared_gradients + (1 - self.beta) * gradients ** 2\n",
        "        return weights - self.learning_rate * gradients / (np.sqrt(self.squared_gradients) + self.epsilon)\n",
        "\n",
        "class Adam(Optimizer):\n",
        "    def __init__(self, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        super().__init__(learning_rate)\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "    def update(self, weights, gradients):\n",
        "        self.t += 1\n",
        "        weights_shape = weights.shape\n",
        "        if weights_shape not in self.m:\n",
        "            self.m[weights_shape] = np.zeros(weights_shape)\n",
        "            self.v[weights_shape] = np.zeros(weights_shape)\n",
        "        self.m[weights_shape] = self.beta1 * self.m[weights_shape] + (1 - self.beta1) * gradients\n",
        "        self.v[weights_shape] = self.beta2 * self.v[weights_shape] + (1 - self.beta2) * gradients ** 2\n",
        "        m_hat = self.m[weights_shape] / (1 - self.beta1 ** self.t)\n",
        "        v_hat = self.v[weights_shape] / (1 - self.beta2 ** self.t)\n",
        "        return weights - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "class Nadam(Adam):\n",
        "    def update(self, weights, gradients):\n",
        "        self.t += 1\n",
        "        self.m = self.beta1 * self.m + (1 - self.beta1) * gradients\n",
        "        self.v = self.beta2 * self.v + (1 - self.beta2) * gradients ** 2\n",
        "        m_hat = (self.beta1 * self.m + (1 - self.beta1) * gradients) / (1 - self.beta1 ** self.t)\n",
        "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
        "        return weights - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, optimizer, activation=sigmoid, activation_derivative=sigmoid_derivative):\n",
        "        self.weights_input_hidden = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.weights_hidden_output = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.optimizer = optimizer\n",
        "        self.activation = activation\n",
        "        self.activation_derivative = activation_derivative\n",
        "    def forward(self, X):\n",
        "        self.hidden_input = np.dot(X, self.weights_input_hidden)\n",
        "        self.hidden_output = self.activation(self.hidden_input)\n",
        "        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output)\n",
        "        self.final_output = softmax(self.final_input)\n",
        "        return self.final_output\n",
        "    def backward(self, X, y_true):\n",
        "        loss_gradient = cross_entropy_derivative(y_true, self.final_output)\n",
        "        d_hidden_output = np.dot(loss_gradient, self.weights_hidden_output.T) * self.activation_derivative(self.hidden_output)\n",
        "        grad_hidden_output = np.dot(self.hidden_output.T, loss_gradient)\n",
        "        grad_input_hidden = np.dot(X.T, d_hidden_output)\n",
        "        self.weights_hidden_output = self.optimizer.update(self.weights_hidden_output, grad_hidden_output)\n",
        "        self.weights_input_hidden = self.optimizer.update(self.weights_input_hidden, grad_input_hidden)\n",
        "    def train(self, X_train, y_train, epochs=10, batch_size=32):\n",
        "        wandb.init(project=\"DA6401-Assignment-1\", id=\"Question-3\", settings=wandb.Settings(init_timeout=300))\n",
        "        loss_history = []\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            for i in range(0, X_train.shape[0], batch_size):\n",
        "                X_batch = X_train[i:i+batch_size]\n",
        "                y_batch = y_train[i:i+batch_size]\n",
        "                predictions = self.forward(X_batch)\n",
        "                loss = cross_entropy(y_batch, predictions)\n",
        "                self.backward(X_batch, y_batch)\n",
        "                epoch_loss += loss\n",
        "            epoch_loss /= (X_train.shape[0] // batch_size)\n",
        "            loss_history.append(epoch_loss)\n",
        "            wandb.log({\"Epoch\": epoch+1, \"Loss\": epoch_loss})\n",
        "            print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}')\n",
        "        wandb.finish()\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.forward(X), axis=1)\n",
        "\n",
        "X_train = np.random.randn(1000, 784)\n",
        "y_train = np.eye(10)[np.random.randint(0, 10, 1000)]\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "nn = NeuralNetwork(input_size=784, hidden_size=128, output_size=10, optimizer=optimizer)\n",
        "nn.train(X_train, y_train, epochs=10, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "_vuDSRhFEOOD",
        "outputId": "f5f719f7-c3b3-4183-8585-0a39563d927d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msubikksha\u001b[0m (\u001b[33msubikksha-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "Epoch 1, Loss: 2.5269\n",
            "Epoch 2, Loss: 1.9832\n",
            "Epoch 3, Loss: 0.6656\n",
            "Epoch 4, Loss: 0.1333\n",
            "Epoch 5, Loss: 0.0463\n",
            "Epoch 6, Loss: 0.0223\n",
            "Epoch 7, Loss: 0.0151\n",
            "Epoch 8, Loss: 0.0114\n",
            "Epoch 9, Loss: 0.0091\n",
            "Epoch 10, Loss: 0.0075\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Loss</td><td>█▆▃▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Loss</td><td>0.00755</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Question-3</strong> at: <a href='https://wandb.ai/subikksha-indian-institute-of-technology-madras/DA6401-Assignment-1/runs/Question-3' target=\"_blank\">https://wandb.ai/subikksha-indian-institute-of-technology-madras/DA6401-Assignment-1/runs/Question-3</a><br> View project at: <a href='https://wandb.ai/subikksha-indian-institute-of-technology-madras/DA6401-Assignment-1' target=\"_blank\">https://wandb.ai/subikksha-indian-institute-of-technology-madras/DA6401-Assignment-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250306_200951-Question-3/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}